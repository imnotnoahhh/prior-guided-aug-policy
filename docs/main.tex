% Template for ICIP-2026 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\raggedbottom  % 允许页面底部不齐，消除 underfull vbox 警告
\usepackage{float}          % 解决 [htbp] 报错

% 减少 figure/table 之间的间距
\setlength{\floatsep}{4pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{6pt plus 1pt minus 1pt}
\setlength{\intextsep}{4pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{3pt}
\setlength{\belowcaptionskip}{0pt}
\setlength{\dblfloatsep}{4pt plus 1pt minus 1pt}
\setlength{\dbltextfloatsep}{6pt plus 1pt minus 1pt}
\usepackage{algorithm}      % 解决 algorithm 环境报错
\usepackage{algpseudocode}  % 解决 algorithmic 报错
\usepackage{booktabs}       % 解决 \toprule, \midrule 报错
\usepackage{xurl}
\usepackage[pdfauthor={},pdftitle={},pdfsubject={},pdfkeywords={}]{hyperref}  % 清理PDF元数据，确保双盲

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{When More is Not Better: Stability-Aware Augmentation for Small-Sample Learning}
%
% Single address.
% ---------------
\name{Author(s) Name(s)}
\address{Author Affiliation(s)}

\begin{document}
\ninept
%
\maketitle
%

\begin{abstract}
Complex data augmentation policies often increase complexity by composing multiple operations. We show that in small-sample regimes, this complexity can introduce substantial performance instability across data splits, undermining reliability in data-scarce applications (e.g., medical imaging). On CIFAR-100 with 100 samples per class, we observe a clear trade-off: while RandAugment attains slightly higher mean accuracy (42.24\% vs.\ 40.74\%), it yields markedly larger fold-to-fold standard deviation ($\sim$50\% higher). To address this, we propose \textbf{SAS} (Stability-aware Augmentation Search), a three-phase protocol that explicitly penalizes instability during augmentation selection. SAS converges to a single, well-tuned operation (ColorJitter) that achieves competitive accuracy while reducing fold variability by 33\% and improving the lower-bound (mean minus std) under disjoint folds. Our results suggest that under data scarcity, \textbf{stability can be a more appropriate objective than blindly increasing augmentation complexity}.
\end{abstract}

\begin{keywords}
Data augmentation, small-sample learning, training stability, augmentation search, image classification
\end{keywords}

\section{Introduction}

When training data is subsampled to fewer than 100 samples for each class, basic assumptions about data augmentation don't hold up. In commonly used tests like ImageNet, the general thought is that ``more is better''---meaning that more operations and bigger magnitudes lead to better generalization. But, our experiments on small-sample CIFAR-100 show that these complex policies bring in a substantial amount of instability. Strong augmentations like RandAugment can compromise the fundamental meaning of the few samples available, causing models to change a lot between different folds of data.

We look into this by asking: are these complex policies needed --- or are they actually harmful --- when there's very little data?

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig1_complexity_gap.png}
    \caption{\textbf{The trade-off between accuracy and stability when working with small samples.} This shows how validation accuracy, training instability (standard deviation $\sigma$), and augmentation complexity relate to each other across different policies. Results are averages from 5 separate folds. A single, well-adjusted operation (Green) finds a good balance between how well it works and how reliable it is.}
    \label{fig:complexity}
    
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig2_pipeline_schematic.png}
    \caption{\textbf{Overview of the augmentation search protocol.} Our three-part process: (I-II) Phase A uses Sobol sampling for quick, low-level testing; (III) Phase B uses the ASHA scheduler to precisely adjust the magnitude ($m^*$) and probability ($p^*$); (IV) Phase C uses a greedy composition with stability constraints, resulting in the choice of a single, robust operation.}
    \label{fig:pipeline}
    
\end{figure*}

In this paper, we conduct a systematic study on CIFAR-100 \cite{krizhevsky2009learning} with only 100 samples per class. We observe that while RandAugment provides a marginal accuracy gain, it introduces significant instability. To address this, we propose \textbf{SAS} (Stability-aware Augmentation Search), a three-phase protocol that explicitly penalizes variance during policy selection. By removing operations that cause variance early on, SAS identifies simple, reliable augmentation policies for small-sample regimes.

SAS explores a full Magnitude-Probability search space, uses multi-level optimization to quickly discard unstable configurations, and employs a greedy selection criterion that explicitly penalizes variance. Surprisingly, this thorough process regularly selects a single operation, rejecting complex combinations. This suggests that when working with small samples, the simplest augmentation that works may be the most reliable.

Our contributions are threefold:
\begin{itemize}
    \item \textbf{Empirical Insight:} We reveal a stability-accuracy trade-off in small-sample augmentation, showing that complex policies introduce high variance that offsets their marginal accuracy gains (Section 4).
    \item \textbf{Methodology:} We propose SAS (Stability-aware Augmentation Search), a three-phase protocol that explicitly penalizes variance using a lower-bound criterion (Mean $-$ Std) for robust policy selection (Section 3).
    \item \textbf{Validation:} Through Stratified 5-fold cross-validation and multi-seed evaluation, we show that single-operation policies offer the best reliability in data-scarce regimes (Section~4).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig3_augmentation_grid.png}
    \caption{\textbf{Candidate Augmentation Operations.} Visualization of the search space pool. Samples are shown with large magnitude parameters for illustrative purposes. Each operation is evaluated for its stability-efficiency trade-off independently before considering composition.}
    \label{fig:augmentations}
\end{figure}

\section{Related Work}

\textbf{Automated Data Augmentation.} AutoAugment \cite{cubuk2019autoaugment} formulated augmentation design as a discrete search problem using Reinforcement Learning. While effective, it was computationally prohibitive. RandAugment \cite{cubuk2020randaugment} simplified this by reducing the search space to two parameters ($N, M$). More recently, TrivialAugment \cite{muller2021trivialaugment} showed that applying a \textit{single random augmentation} per image can match or exceed complex policies---a finding that resonates with our conclusions about simplicity. However, these methods were developed and tuned on large-scale datasets (ImageNet, full CIFAR-100). Our work demonstrates that the ``complexity leads to performance'' assumption fails when data is scarce, where stability becomes the primary concern.

\textbf{Understanding Augmentation Effects.} Recent theoretical work \cite{chen2024spectral} reveals that data augmentation operates through implicit spectral regularization, manipulating the eigenvalue structure of data covariance matrices. This explains why augmentations can both help and hurt generalization depending on the data regime. Yang et al.~\cite{yang2023sample} show that augmentation consistency regularization is inherently more sample-efficient than standard augmented training. Our empirical findings align with these insights: in small-sample regimes, simpler augmentations provide more stable regularization without the variance introduced by complex policies.

\textbf{Small-Sample Learning.} When data is scarce, models suffer from high variance. While transfer learning is common, it doesn't apply when domains mismatch, forcing training from scratch. In this setting, aggressive augmentations (like Cutout \cite{devries2017improved}) can destroy limited semantic information. Our work takes a ``Data-Centric AI'' perspective, adapting the augmentation strategy to the data regime rather than assuming one-size-fits-all policies.

\section{Augmentation Search Protocol}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{strategic_collapse.png}
    \caption{\textbf{Analysis of Policy Selection}: As the number of operations in the policy increases (Greedy Search), the validation variance greatly increases (red bars), often canceling out small gains in accuracy. The search finds that a single, well-adjusted operation ($N=1$) gives the best trade-off between accuracy and stability for this small-sample regime.}
    \label{fig:collapse}
\end{figure}

\subsection{Search Space Definition}
We consider a search space of $K=8$ candidate operations, each parameterized by Magnitude ($m \in [0,1]$) and Probability ($p \in [0,1]$). Table~\ref{tab:op_mapping} shows how $m$ maps to physical parameters.

\begin{table}[!t]
    \centering
    \caption{Magnitude to physical parameter mapping for $K=8$ candidate operations.}
    \label{tab:op_mapping}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lll}
            \toprule
            Operation & Parameter & Mapping ($m \in [0,1]$) \\
            \midrule
            ColorJitter & brightness, contrast, saturation & $[0, 0.8m]$, hue$=0$ \\
            RandomGrayscale & probability & $[0, 0.5m]$ \\
            GaussianNoise & $\sigma$ & $[0, 0.1m]$ \\
            RandomResizedCrop & scale\_min & $[1.0 - 0.75m, 1.0]$ \\
            RandomRotation & degrees & $[0, 30m]$ \\
            GaussianBlur & $\sigma$ & $[0.1, 0.1 + 1.9m]$ \\
            RandomErasing & scale & $[0.02, 0.02 + 0.38m]$ \\
            RandomPerspective & distortion & $[0, 0.5m]$ \\
            \bottomrule
        \end{tabular}
    }
\end{table}

We define augmentation complexity $C$ as the expected number of transformations applied per sample: $C = \sum_{i=1}^{K} p_i$. For RandAugment, $C = N$ (fixed). For SAS, $C \leq 1$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig4_search_space_colorjitter.png}
    \caption{\textbf{Empirical Search Manifold.} Validation accuracy distribution over the continuous 2D search space (Magnitude $m$, Probability $p$) for \texttt{ColorJitter}, obtained during Phase A screening. The red marker denotes the high-performance region identified for fine-tuning in Phase B.}
    \label{fig:search_space}
\end{figure}

\subsection{Search Protocol}
To handle the stability-accuracy trade-off, we didn't do a normal grid search. Instead, we made a three-stage funnel.

\textbf{Phase A: Screening.} We started with a quick scan using Sobol sequences. This let us quickly drop unstable operations (like high-magnitude geometric distortions) before expending significant computing power.

\textbf{Phase B: Tuning.} For the candidates that pass this screen, we apply ASHA \cite{li2020system} to fine-tune them. This step dynamically focuses our computing budget on the best performers, discarding weak configurations early. This precise tuning of $(m, p)$ was done at a fraction of the cost of a full grid search.

\textbf{Phase C: Composition.} Finally, we consider composition (Algorithm \ref{alg:sas}, Lines 17-23). We use a stability-aware greedy strategy with selection criterion: a candidate operation is accepted only if $\text{Acc}_{trial} > \text{Acc}_{best} + \alpha \cdot \sigma_{trial}$, where $\alpha = 1.0$ penalizes variance. This is equivalent to maximizing the lower bound (Mean $-$ Std). Interestingly, this check almost always rejected multi-operation policies, repeatedly converging on single, stable transformations.

\textbf{Design Rationale.} The key insight behind SAS is that in small-sample regimes, the variance introduced by complex augmentation policies often outweighs their accuracy benefits. By explicitly penalizing variance in our selection criterion, we prioritize policies that provide consistent performance across different data splits. This is particularly important for safety-critical applications where worst-case performance matters more than average-case gains.

\begin{algorithm}[htbp]
\caption{SAS: Stability-aware Augmentation Search}
\label{alg:sas}
\begin{algorithmic}[1]
\Require Candidate Ops $\mathcal{O} = \{o_1, ..., o_K\}$, Trade-off $\alpha$
\Ensure Optimal policy $\pi^*$

\State \textbf{Phase A: Screening}
\For{$o \in \mathcal{O}$}
    \State Sample $(m, p)$ pairs using Sobol sequence
    \State $\mu_o, \sigma_o \leftarrow$ Evaluate with 5-fold CV (quick training)
    \State Record promising $(o, m, p)$ configurations
\EndFor

\State \textbf{Phase B: Tuning}
\For{top candidates from Phase A}
    \State $(m^*, p^*) \leftarrow$ Fine-tune with ASHA scheduler
    \State $\mu^*, \sigma^* \leftarrow$ Evaluate with full training
\EndFor

\State \textbf{Phase C: Composition with Stability Constraint}
\State $\pi^* \leftarrow$ Best single operation from Phase B
\For{candidate $op$ to add}
    \State $\mu_{trial}, \sigma_{trial} \leftarrow$ Evaluate $\pi^* \cup \{op\}$
    \If{$\mu_{trial} > \mu_{best} + \alpha \cdot \sigma_{trial}$} \Comment{Variance penalty}
        \State $\pi^* \leftarrow \pi^* \cup \{op\}$
    \EndIf
\EndFor
\State \Return $\pi^*$ \Comment{Typically converges to single operation}
\end{algorithmic}
\end{algorithm}

\section{Experiments}

\subsection{Setup}
\textbf{Dataset Splits.} We use CIFAR-100 subsampled to 100 images per class (10,000 total). We evaluate methods via Stratified 5-fold cross-validation on this fixed subset, ensuring no sample appears in more than one fold. Each fold uses 80\% for training (80 samples/class, 8,000 total) and 20\% for validation (20 samples/class, 2,000 total). We report mean $\pm$ std over the 5 folds, where std reflects sensitivity to disjoint data splits.

\textbf{Training.} We use ResNet-18 \cite{he2016deep} trained from scratch for 200 epochs (SGD, momentum 0.9, weight decay set to $0.01$, and a batch size of 128). All experiments were performed on a single NVIDIA A10 GPU. The search process completes in $<$4 hours total (Phase A: 1h, Phase B: 2.5h).

\textbf{Best Policy Found.} SAS identifies a single operation: \textbf{ColorJitter} with Magnitude $m=0.2575$, Probability $p=0.4239$. We tie brightness, contrast, and saturation to $m$: $(b,c,s,h)=(m,m,m,0)$. Random seeds used: $\{42, 100, 2024, 7, 99\}$.

\subsection{Main Results}
Table \ref{tab:main_results} compares SAS against standard baselines.

\begin{table}[htbp]
    \centering
    \caption{Comparison on CIFAR-100 (100 samples/class). All results are mean $\pm$ std over 5 disjoint folds (Stratified K-Fold) on the fixed 100-shot subset. \textbf{Std Dev} measures fold-to-fold variance.}
    \label{tab:main_results}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccc}
            \toprule
            Policy & Val Acc \% & Std Dev $\downarrow$ & Min Acc & Complexity \\
            \midrule
            Baseline (S0) & 39.90 & 1.01 & 38.30 & Low \\
            Baseline-NoAug & 29.08 & 3.30 & 23.90 & None \\
            RandAugment & \textbf{42.24} & 1.17 & \textbf{40.60} & High (N=2) \\
            Cutout & 36.26 & 1.23 & 34.50 & Med \\
            \midrule
            \textbf{SAS (ColorJitter)} & 40.74 & \textbf{0.78} & 40.10 & \textbf{Low} \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Analysis}
\textbf{The Stability Argument.} While RandAugment achieves +1.5\% higher mean accuracy, this comes at the cost of \textbf{predictability}. As shown in Figure \ref{fig:stability}, RandAugment exhibits 50\% higher variance ($\sigma=1.17$ vs.\ $0.78$). This standard deviation represents \textbf{Fold Variance}---sensitivity to how training data is split. While RandAugment's worst-case fold (40.60\%) still exceeds SAS (40.10\%), practitioners cannot reliably predict which fold will perform well. We separately verified \textbf{Seed Variance} (sensitivity to initialization) in CIFAR-10 experiments (see supplemental material). In safety-critical applications (e.g., medical imaging), unpredictable performance across data splits is unacceptable; a method with tighter variance bounds is preferred even at a small accuracy cost.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig5_stability_boxplot.png}
    \caption{\textbf{Distribution of validation accuracy across 5 disjoint folds.} Comparison of Baseline, RandAugment, and the SAS. While RandAugment exhibits higher peak performance, it suffers from significant variance, whereas the SAS achieves competitive results with a 33\% reduction in standard deviation ($\sigma=0.78$ vs $1.17$).}
    \label{fig:stability}
\end{figure}

\textbf{Accuracy-Stability Trade-off.} To measure this trade-off, we define augmentation complexity $C$ as the expected number of transformations applied per sample: $C = \sum p_i$. For RandAugment, complexity is the fixed number of operations applied $N=2$. For our SAS, $C \le 1$. Figure \ref{fig:complexity} (Introduction) shows the main finding.

\textbf{Fair Comparison: Tuning RandAugment.} A common critique is that RandAugment might outperform if properly tuned. To address this, we performed a random search for optimal RandAugment parameters ($N \in \{1,2,3\}$, $M \in \{1,...,14\}$) on the same 100-shot setup. We sampled 10 random configurations, trained each for 40 epochs on Fold 0 for quick screening, then fully trained the best configuration (N=1, M=2) for 200 epochs. This achieved only 35.30\% validation accuracy---significantly lower than both default RandAugment (42.24\%) and our SAS (40.74\%). We use the official \texttt{torchvision.transforms.RandAugment} implementation with identical operation pool to ensure fair comparison.

Why do complex strategies fail here? Two reasons stand out. First, \textbf{Validation Overfitting}: with scarce data, the search algorithm easily exploits noise in the small validation set, selecting parameters that fail to generalize. Second, \textbf{Loss of Inductive Biases}: the default RandAugment parameters ($N=2, M=9$) effectively serve as a strong prior derived from large-scale datasets (ImageNet). Blindly searching from scratch discards this inductive bias. SAS works because it stays within a safe ``semantic corridor,'' modifying images enough to teach invariance without confusing the model with extraneous noise.

\textbf{Semantic Preservation.} We hypothesize that the high variance of complex strategies stems from semantic destruction. To investigate this, we quantified the "destructiveness" of augmentation policies using SSIM (Structural Similarity) and LPIPS (Perceptual Similarity) metrics, averaged over 500 (Original, Augmented) validation image pairs. Results indicate that the SAS maintains high structural integrity (SSIM: $\approx0.196$, LPIPS: $\approx0.091$), comparable to the minimal Baseline (SSIM: $\approx0.198$, LPIPS: $\approx0.084$). Note that absolute SSIM values are low due to geometric transformations causing spatial misalignment, so we focus on relative degradation. In contrast, RandAugment significantly distorts image structure (SSIM: $\approx0.147$, LPIPS: $\approx0.124$). This quantitative evidence supports our hypothesis: the SAS improves performance by preserving semantic content, while RandAugment's gains come at the cost of aggressive distortion, leading to instability. Figure \ref{fig:failure_cases} provides visual evidence of this semantic destruction.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig8_destructiveness.png}
    \vspace{-0.5em}
    \caption{\textbf{Semantic Preservation Analysis.} Comparison of destructive metrics (SSIM and LPIPS) across strategies. The SAS (Green) maintains structural similarity (SSIM $\uparrow$) and perceptual quality (LPIPS $\downarrow$) comparable to the Baseline, whereas RandAugment (Red) introduces significant semantic distortion. Error bars denote standard deviation.}
    \label{fig:destructiveness}
\end{figure}

\vspace{-1em}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{fig_failure_cases_teaser.png}
    \caption{\textbf{Visual Comparison of Augmentation Effects.} Random validation samples (seed=42) showing Original $\rightarrow$ RandAugment (2 samples) $\rightarrow$ SAS. RandAugment introduces black borders, color inversion, and severe distortion (low SSIM), leading to misclassification (red). SAS preserves semantic content with higher SSIM.}
    \label{fig:failure_cases}
\end{figure}

\textbf{Scaling Behavior.} We report an additional shot-scaling study in the supplemental material to illustrate scaling trends and potential failure modes under a different protocol.

\subsection{Ablation: The Necessity of Magnitude Search}
A natural question is whether the full 2D search over $(m, p)$ is necessary, or if a simpler approach would suffice. We investigated this by fixing $p=0.5$ for ColorJitter and searching only for $m$ using 10 Sobol samples.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig7_ablation_magnitude.png}
    \caption{\textbf{Sensitivity of validation accuracy to Magnitude $m$ (fixed $p=0.5$).} The 4.2\% accuracy gap between optimal ($m \approx 0.44$) and defaults ($m \approx 0.54$) highlights the necessity of Magnitude tuning.}
    \label{fig:ablation}
\end{figure}

The results show clear performance variance driven by magnitude changes: accuracy ranges from 20\% to 34\% depending on $m$. This validates our design choice of a 2D search space $(m, p)$ over a simplified 1D search. Even with a fixed probability, the magnitude parameter critically affects performance, justifying the additional search dimension.

\subsection{Generalization to CIFAR-10 (50-shot)}
Having validated the search protocol design, we next evaluate whether our findings generalize to different datasets. We performed an experiment on CIFAR-10 subsampled to 50 images per class (500 images total), using the same training protocol.

\begin{table}[htbp]
    \centering
    \caption{Results on CIFAR-10 (50-shot). SAS matches RandAugment while maintaining zero variance across folds.}
    \label{tab:cifar10}
    \begin{tabular}{lcc}
        \toprule
        Method & Mean Acc (\%) & Std Dev \\
        \midrule
        Baseline & 45.20 & 18.66 \\
        RandAugment & 50.00 & 0.00 \\
        \textbf{SAS} & 50.00 & 0.00 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig6_cifar10_generalization.png}
    \caption{\textbf{Cross-dataset Generalization.} Performance on CIFAR-10 (50-shot) over 5 folds. Both SAS and RandAugment converge to 50\% with zero variance, while Baseline shows extreme instability.}
    \label{fig:cifar10_gen}
\end{figure}

The zero variance reflects performance saturation in this extreme regime. We verified this is not an artifact by testing 3 random seeds---all converged to exactly 50.00\%. This cross-dataset consistency suggests that our findings are not specific to CIFAR-100 but reflect a general phenomenon in small-sample learning.

\section{Limitations and Future Work}

\textbf{Limitations.} Our study is limited to (1) convolutional architectures (ResNet-18) trained from scratch, (2) CIFAR-100/10 benchmarks, and (3) the specific 100-shot regime. Whether similar conclusions hold for Vision Transformers, which often require stronger regularization due to their lack of inductive biases, remains to be investigated. Additionally, the same validation folds used for policy selection are also used for final reporting. To mitigate selection bias, we use Stratified 5-fold cross-validation with disjoint folds and report results across multiple random seeds. Our core claim concerns \textit{relative stability} (variance comparison) rather than absolute accuracy. We report fold-to-fold variance on CIFAR-100 as the primary stability metric; as a sanity check for initialization sensitivity, we additionally vary random seeds on CIFAR-10 and observe consistent conclusions (Section~\ref{tab:cifar10}).

\textbf{Future Work.} We identify three promising directions:
\begin{itemize}
    \item Extending SAS to Vision Transformers and self-supervised learning paradigms;
    \item Validating on real-world small-sample domains (e.g., medical imaging, satellite imagery);
    \item Adopting nested cross-validation where inner folds are used for search and outer folds for evaluation.
\end{itemize}

\section{Conclusion}
Through a systematic empirical study on CIFAR-100 with 100 samples per class, we find that complex strategies stop working well when data is scarce. Our multi-phase search protocol identifies a single, well-tuned operation (ColorJitter) that achieves competitive accuracy with 33\% lower standard deviation compared to RandAugment under disjoint StratifiedKFold evaluation. These findings suggest that, in our 100-shot CIFAR-100 scenario, augmentation design should prioritize stability over complexity.

\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
