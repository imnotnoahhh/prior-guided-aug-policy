# When More is Not Better: Rethinking Data Augmentation under Small-Sample Regimes
# å°æ ·æœ¬åœºæ™¯ä¸‹çš„å¢žå¼ºç­–ç•¥å†æ€è€ƒï¼šå¤šæœªå¿…æ›´å¥½

[![Python 3.14](https://img.shields.io/badge/python-3.14-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.5](https://img.shields.io/badge/pytorch-2.5-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Official PyTorch Implementation** of the paper: *"When More is Not Better: Rethinking Data Augmentation under Small-Sample Regimes"* (WACV/BMVC Submission Target).
> 
> **Official Implementation**: This study reveals the counter-intuitive finding that increasing augmentation complexity often yields diminishing returns in small-sample regimes.

---

## ðŸ“– Abstract / æ‘˜è¦

**English**:  
Data augmentation is critical for deep learning in data-scarce regimes. While complex automated strategies like RandAugment achieve state-of-the-art results on large datasets, their efficacy in small-sample settings (e.g., CIFAR-100, 100-shot) remains under-explored. We find that blindly increasing augmentation complexity yields diminishing returns while significantly increasing training instability. 

Through a multi-phase search protocol combining Sobol sampling and ASHA scheduling, we discover that a single, well-tuned operation (ColorJitter) can achieve competitive accuracy (40.74%) compared to RandAugment (42.24%) but with **significantly lower variance (Std: 0.78 vs 1.17)**. Our findings suggest that in small-sample regimes, augmentation design should prioritize stability over complexity.

**ä¸­æ–‡**:  
åœ¨æ•°æ®åŒ®ä¹çš„åœºæ™¯ä¸‹ï¼Œæ•°æ®å¢žå¼ºè‡³å…³é‡è¦ã€‚è™½ç„¶åƒ RandAugment è¿™æ ·çš„è‡ªåŠ¨å¢žå¼ºç­–ç•¥åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨å°æ ·æœ¬ï¼ˆå¦‚ CIFAR-100 æ¯ç±» 100 å¼ ï¼‰åœºæ™¯ä¸‹ï¼Œç›²ç›®å¢žåŠ å¢žå¼ºæ“ä½œçš„å¤æ‚åº¦ä¸ä»…æ”¶ç›Šé€’å‡ï¼Œè¿˜ä¼šæ˜¾è‘—å¢žåŠ è®­ç»ƒçš„ä¸ç¨³å®šæ€§ã€‚

é€šè¿‡ä¸€å¥—ç»“åˆ Sobol é‡‡æ ·å’Œ ASHA è°ƒåº¦çš„å¤šé˜¶æ®µæœç´¢æµç¨‹ï¼Œæˆ‘ä»¬å‘çŽ°å•ä¸€çš„ã€è°ƒä¼˜è‰¯å¥½çš„æ“ä½œï¼ˆå¦‚ ColorJitterï¼‰èƒ½è¾¾åˆ°ä¸Ž RandAugment ç›¸å½“çš„å‡†ç¡®çŽ‡ï¼ˆ40.74% vs 42.24%ï¼‰ï¼ŒåŒæ—¶**æ–¹å·®æ˜¾è‘—é™ä½Žï¼ˆStd: 0.78 vs 1.17ï¼‰**ã€‚æˆ‘ä»¬çš„å‘çŽ°è¡¨æ˜Žï¼Œåœ¨å°æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå¢žå¼ºç­–ç•¥è®¾è®¡åº”ä¼˜å…ˆè€ƒè™‘ç¨³å®šæ€§è€Œéžå¤æ‚åº¦ã€‚

---

## ðŸ“Š Key Results / æ ¸å¿ƒç»“æžœ

Experiments conducted on CIFAR-100 (100 samples/class), ResNet-18, 5-Fold Cross-Validation.

| Method | Mean Acc (%) | Stability (Std) | Complexity | Note |
| :--- | :---: | :---: | :---: | :--- |
| Baseline (S0) | 39.90 | 1.01 | Low | Basic Crop/Flip |
| **RandAugment** (N=2,M=9) | **42.24** | 1.17 | High | **Unstable** (High Variance) |
| **Tuned RandAugment** (N=1,M=2)| 35.30 | N/A | Low | Tuning fails (Underfitting) |
| **Single-Op (ColorJitter)** | 40.74 | **0.78** | **Low** | **Most Stable and Reliable** |

### Why Single-Op? / Why single-operation policy?
1.  **Zero Variance in Stability Check**: Verified to converge consistently (50.00% Â± 0.00%) across 3 random seeds in 50-shot scenarios.
2.  **High Semantic Fidelity**: LPIPS score (0.091) is comparable to baseline, unlike RandAugment (0.124) which distorts images heavily.
3.  **Efficiency**: Search cost is only ~4 GPU hours, finding the optimal policy without expensive reinforcement learning.

---

## ðŸ›  Installation / å®‰è£…

```bash
# Clone the repository
git clone https://github.com/yourusername/prior-guided-aug.git
cd prior-guided-aug

# Create Conda environment
conda env create -f environment.yml
conda activate pga

# (Optional) Verify installation
python -c "import torch; print(torch.cuda.is_available())"
```

---

## ðŸš€ Quick Start / å¿«é€Ÿå¼€å§‹

### 1. Reproduction (One-Click) / ä¸€é”®å¤çŽ°
Run the full pipeline (Phase A -> B -> C -> D) on a single GPU:

```bash
bash scripts/train_single_gpu.sh
```

### 2. Supplementary Experiments / è¡¥å……å®žéªŒ (Paper Revision)
Reproduce the specific proofs for stability and fairness:

```bash
# Verify Semantic Preservation (Destructiveness)
python scripts/calculate_destructiveness.py

# Verify Zero Variance (Stability)
python scripts/run_stability_check.py

# Verify Tuned RandAugment Failure (Fairness)
python scripts/run_tuning_randaugment.py  # Search
python scripts/run_final_tuned_ra.py      # Validation

# Verify Policy Selection (Figure 2)
python scripts/plot_strategic_collapse.py


```

### 3. Visualization / ç»˜å›¾
Generate all figures used in the paper:

```bash
python scripts/generate_paper_figures.py
```
Output: `outputs/figures/`

---

## ðŸ“‚ Project Structure / é¡¹ç›®ç»“æž„

```
.
â”œâ”€â”€ src/                # Core implementation
â”‚   â”œâ”€â”€ augmentations.py  # Search space & Augmentation logic
â”‚   â”œâ”€â”€ dataset.py        # CIFAR-100 Subsampled dataset
â”‚   â””â”€â”€ models.py         # ResNet-18
â”œâ”€â”€ scripts/            # Experiment scripts
â”‚   â”œâ”€â”€ train_single_gpu.sh      # Full pipeline runner
â”‚   â”œâ”€â”€ calculate_destructiveness.py # LPIPS/SSIM analysis
â”‚   â””â”€â”€ generate_paper_figures.py    # Plotting tools
â”œâ”€â”€ docs/               # Documentation
â”‚   â”œâ”€â”€ paper_draft.tex   # LaTeX draft
â”‚   â””â”€â”€ repro_guide.md    # Detailed guide
â”œâ”€â”€ outputs/            # Experiment results (Auto-generated)
â””â”€â”€ logs/               # Training logs
```

---

## ðŸ“œ Citation / å¼•ç”¨

If you find this work useful, please stay tuned! The citation will be updated upon acceptance.

<!--
```bibtex
@article{qin2026prior,
  title={When More is Not Better: Rethinking Data Augmentation under Small-Sample Regimes},
  author={Qin, Fuyao},
  journal={arXiv preprint},
  year={2026}
}
```
-->

## ðŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.
